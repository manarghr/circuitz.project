{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1a1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manar\\Desktop\\nassima\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4339f",
   "metadata": {},
   "source": [
    "# Download latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a8fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\manar\\.cache\\kagglehub\\datasets\\astraszab\\facial-expression-dataset-image-folders-fer2013\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"astraszab/facial-expression-dataset-image-folders-fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7134fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FER_ROOT: C:\\Users\\manar\\.cache\\kagglehub\\datasets\\astraszab\\facial-expression-dataset-image-folders-fer2013\\versions\\1\n",
      "DATA_DIR: C:\\Users\\manar\\.cache\\kagglehub\\datasets\\astraszab\\facial-expression-dataset-image-folders-fer2013\\versions\\1\\data\n",
      "Subfolders at DATA_DIR: ['test', 'train', 'val']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path = Path(path)\n",
    "DATA_DIR = path / \"data\"\n",
    "\n",
    "print(\"FER_ROOT:\", path)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Subfolders at DATA_DIR:\", [p.name for p in DATA_DIR.iterdir() if p.is_dir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc07b",
   "metadata": {},
   "source": [
    "# FER2013 numeric folder -> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f937c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "FER_ID2NAME = {'0':'angry','1':'disgust','2':'fear','3':'happy','4':'sad','5':'surprise','6':'neutral'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97930534",
   "metadata": {},
   "source": [
    "# We keep only these folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799ccba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping: {'3': 'happy', '4': 'sad', '6': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "KEEP_IDS = {'3':'happy', '4':'sad', '6':'neutral'}     # folder names in source\n",
    "NAME2TARGET = {'happy':0, 'neutral':1, 'sad':2}        # our label ids\n",
    "print(\"Keeping:\", KEEP_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3983db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),   # FER2013 is grayscale\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafef35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_IDS     = {'3':'happy', '4':'sad', '6':'neutral'}\n",
    "NAME2TARGET  = {'happy':0, 'neutral':1, 'sad':2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46846363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),   # FER2013 is grayscale\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fed419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts: Counter({0: 7215, 1: 4965, 2: 4830})\n",
      "Val   counts: Counter({0: 895, 2: 653, 1: 607})\n",
      "class_to_idx: {'happy': 0, 'neutral': 1, 'sad': 2}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "KEEP_IDS     = {'3': 'happy', '4': 'sad', '6': 'neutral'}\n",
    "NAME2TARGET  = {'happy': 0, 'neutral': 1, 'sad': 2}\n",
    "\n",
    "def make_three_class_dataset(root: Path, tfms):\n",
    "    ds = datasets.ImageFolder(root=str(root), transform=tfms)\n",
    "\n",
    "    new_samples, new_targets = [], []\n",
    "\n",
    "    # each item in ds.samples is (img_path: str, class_idx: int)\n",
    "    for img_path, _ in ds.samples:\n",
    "        cls_folder = Path(img_path).parent.name  # e.g. '0'..'6'\n",
    "        if cls_folder in KEEP_IDS:\n",
    "            label_name = KEEP_IDS[cls_folder]        # 'happy'/'sad'/'neutral'\n",
    "            label_id   = NAME2TARGET[label_name]     # 0/1/2\n",
    "            new_samples.append((img_path, label_id))\n",
    "            new_targets.append(label_id)\n",
    "\n",
    "    # keep ImageFolder internals consistent\n",
    "    ds.samples = new_samples\n",
    "    ds.imgs = new_samples           # alias used by older torchvision\n",
    "    ds.targets = new_targets\n",
    "    ds.classes = ['happy', 'neutral', 'sad']\n",
    "    ds.class_to_idx = {'happy': 0, 'neutral': 1, 'sad': 2}\n",
    "\n",
    "    return ds\n",
    "\n",
    "#Make sure DATA_DIR, train_tfms, val_tfms are defined earlier\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR   = DATA_DIR / \"val\"\n",
    "\n",
    "train_ds = make_three_class_dataset(TRAIN_DIR, train_tfms)\n",
    "val_ds   = make_three_class_dataset(VAL_DIR,   val_tfms)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Train counts:\", Counter(train_ds.targets))\n",
    "print(\"Val   counts:\", Counter(val_ds.targets))\n",
    "print(\"class_to_idx:\", train_ds.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460d00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_weights (for CE): [1.0, 1.453172206878662, 1.4937888383865356]\n",
      "BATCH_SIZE = 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#class weights for sampler & loss\n",
    "counts = Counter(train_ds.targets)          # {0:..., 1:..., 2:...}\n",
    "max_c = max(counts.values())\n",
    "loss_weights = torch.tensor(\n",
    "    [max_c / counts[i] for i in range(3)],  # order: 0=happy,1=neutral,2=sad\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "#WeightedRandomSampler so each mini-batch is balanced-ish\n",
    "sample_weights = torch.tensor([loss_weights[t] for t in train_ds.targets],\n",
    "                              dtype=torch.float32)\n",
    "sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                num_samples=len(sample_weights),\n",
    "                                replacement=True)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          sampler=sampler, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"loss_weights (for CE):\", loss_weights.tolist())\n",
    "print(\"BATCH_SIZE =\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69519ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manar\\AppData\\Local\\Temp\\ipykernel_25752\\3103928000.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
      "C:\\Users\\manar\\AppData\\Local\\Temp\\ipykernel_25752\\3103928000.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
      "C:\\Users\\manar\\AppData\\Local\\Temp\\ipykernel_25752\\3103928000.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/15 | train acc 0.553 loss 0.9194 | val acc 0.573 loss 0.9155\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.573)\n",
      "Epoch 02/15 | train acc 0.599 loss 0.8487 | val acc 0.558 loss 0.9303\n",
      "ðŸ”“ Unfroze backbone and reset optimizer.\n",
      "Epoch 02/15 | train acc 0.599 loss 0.8487 | val acc 0.558 loss 0.9303\n",
      "ðŸ”“ Unfroze backbone and reset optimizer.\n",
      "Epoch 03/15 | train acc 0.737 loss 0.6134 | val acc 0.727 loss 0.6219\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.727)\n",
      "Epoch 03/15 | train acc 0.737 loss 0.6134 | val acc 0.727 loss 0.6219\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.727)\n",
      "Epoch 04/15 | train acc 0.815 loss 0.4649 | val acc 0.773 loss 0.5577\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.773)\n",
      "Epoch 04/15 | train acc 0.815 loss 0.4649 | val acc 0.773 loss 0.5577\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.773)\n",
      "Epoch 05/15 | train acc 0.850 loss 0.3901 | val acc 0.764 loss 0.5985\n",
      "Epoch 05/15 | train acc 0.850 loss 0.3901 | val acc 0.764 loss 0.5985\n",
      "Epoch 06/15 | train acc 0.889 loss 0.3033 | val acc 0.782 loss 0.6095\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.782)\n",
      "Epoch 06/15 | train acc 0.889 loss 0.3033 | val acc 0.782 loss 0.6095\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.782)\n",
      "Epoch 07/15 | train acc 0.911 loss 0.2480 | val acc 0.764 loss 0.7115\n",
      "Epoch 07/15 | train acc 0.911 loss 0.2480 | val acc 0.764 loss 0.7115\n",
      "Epoch 08/15 | train acc 0.934 loss 0.1831 | val acc 0.768 loss 0.7087\n",
      "Epoch 08/15 | train acc 0.934 loss 0.1831 | val acc 0.768 loss 0.7087\n",
      "Epoch 09/15 | train acc 0.943 loss 0.1588 | val acc 0.787 loss 0.7420\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.787)\n",
      "Epoch 09/15 | train acc 0.943 loss 0.1588 | val acc 0.787 loss 0.7420\n",
      "âœ… Saved best to emotion_model_best.pt (val_acc=0.787)\n",
      "Epoch 10/15 | train acc 0.955 loss 0.1250 | val acc 0.787 loss 0.8481\n",
      "Epoch 10/15 | train acc 0.955 loss 0.1250 | val acc 0.787 loss 0.8481\n",
      "Epoch 11/15 | train acc 0.965 loss 0.1050 | val acc 0.763 loss 0.8837\n",
      "Epoch 11/15 | train acc 0.965 loss 0.1050 | val acc 0.763 loss 0.8837\n",
      "Epoch 12/15 | train acc 0.970 loss 0.0873 | val acc 0.783 loss 0.9036\n",
      "Epoch 12/15 | train acc 0.970 loss 0.0873 | val acc 0.783 loss 0.9036\n",
      "Epoch 13/15 | train acc 0.973 loss 0.0781 | val acc 0.775 loss 0.9059\n",
      "Epoch 13/15 | train acc 0.973 loss 0.0781 | val acc 0.775 loss 0.9059\n",
      "Epoch 14/15 | train acc 0.976 loss 0.0673 | val acc 0.777 loss 1.0136\n",
      "Epoch 14/15 | train acc 0.976 loss 0.0673 | val acc 0.777 loss 1.0136\n",
      "Epoch 15/15 | train acc 0.980 loss 0.0585 | val acc 0.781 loss 1.0574\n",
      "Done. Best val acc: 0.7874709976798144\n",
      "Epoch 15/15 | train acc 0.980 loss 0.0585 | val acc 0.781 loss 1.0574\n",
      "Done. Best val acc: 0.7874709976798144\n"
     ]
    }
   ],
   "source": [
    "# ===== Training: MobileNetV3-Large (3 classes) =====\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 15\n",
    "BEST_PATH = \"emotion_model_best.pt\"\n",
    "\n",
    "# 1) Build model\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "in_feats = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(in_feats, 3)   # happy/neutral/sad\n",
    "\n",
    "# (optional) warmup: freeze backbone for first 2 epochs\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# 2) Loss / optimizer / scheduler\n",
    "ce_weights = (loss_weights / loss_weights.min()).to(DEVICE)  # normalize a bit\n",
    "criterion = nn.CrossEntropyLoss(weight=ce_weights)\n",
    "\n",
    "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                  lr=3e-4, weight_decay=1e-4)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    for xb, yb in dl:\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        if DEVICE == \"cuda\":\n",
    "            xb = xb.to(memory_format=torch.channels_last)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        loss_sum += loss.item() * yb.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "    return loss_sum / max(1, tot), correct / max(1, tot)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # unfreeze after warmup\n",
    "    if epoch == 3:\n",
    "        for p in model.features.parameters():\n",
    "            p.requires_grad = True\n",
    "        opt = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "        print(\"ðŸ”“ Unfroze backbone and reset optimizer.\")\n",
    "\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "    sched.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train acc {tr_acc:.3f} loss {tr_loss:.4f} | \"\n",
    "          f\"val acc {va_acc:.3f} loss {va_loss:.4f}\")\n",
    "\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"âœ… Saved best to {BEST_PATH} (val_acc={best_acc:.3f})\")\n",
    "\n",
    "print(\"Done. Best val acc:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb8910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved:\n",
      " - state_dict : artifacts\\best_model.pt\n",
      " - TorchScript(trace): artifacts\\emotion_mnv3_ts.pt\n",
      " - ONNX       : artifacts\\emotion_mnv3.onnx\n",
      " - labels     : artifacts\\labels.json\n",
      "Sanity forward OK. logits shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "#==== SAVE STEP (device-safe) ====\n",
    "import os, json, copy, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "OUT_DIR = \"artifacts\"\n",
    "STATE_DICT_OUT = os.path.join(OUT_DIR, \"best_model.pt\")\n",
    "TS_OUT         = os.path.join(OUT_DIR, \"emotion_mnv3_ts.pt\")\n",
    "ONNX_OUT       = os.path.join(OUT_DIR, \"emotion_mnv3.onnx\")\n",
    "LABELS_OUT     = os.path.join(OUT_DIR, \"labels.json\")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "LABELS   = [\"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "if 'model' not in globals():\n",
    "    raise RuntimeError(\"No model in memory. Run this cell right after training.\")\n",
    "\n",
    "#1) Save standard PyTorch weights (from your current device, GPU or CPU)\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), STATE_DICT_OUT)\n",
    "\n",
    "#2) For exports, move a COPY to CPU to avoid device mismatches\n",
    "export_model = copy.deepcopy(model).to(\"cpu\").eval()\n",
    "\n",
    "#3) Try TorchScript (trace). If tracing has issues, fall back to scripting.\n",
    "example = torch.randn(1, 3, IMG_SIZE, IMG_SIZE, device=\"cpu\")\n",
    "try:\n",
    "    ts_model = torch.jit.trace(export_model, example)\n",
    "    ts_model.save(TS_OUT)\n",
    "    ts_mode = \"trace\"\n",
    "except Exception as e:\n",
    "    print(\"[-] TorchScript trace failed, trying script:\", e)\n",
    "    ts_model = torch.jit.script(export_model)\n",
    "    ts_model.save(TS_OUT)\n",
    "    ts_mode = \"script\"\n",
    "\n",
    "#4) Export ONNX from the CPU copy\n",
    "torch.onnx.export(\n",
    "    export_model, example, ONNX_OUT,\n",
    "    input_names=[\"input\"], output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "    opset_version=12\n",
    ")\n",
    "\n",
    "#5) Save labels\n",
    "with open(LABELS_OUT, \"w\") as f:\n",
    "    json.dump(LABELS, f)\n",
    "\n",
    "#6) Sanity forward on CPU copy\n",
    "with torch.no_grad():\n",
    "    y = export_model(example)\n",
    "print(f\"âœ… Saved:\\n - state_dict : {STATE_DICT_OUT}\\n - TorchScript({ts_mode}): {TS_OUT}\\n - ONNX       : {ONNX_OUT}\\n - labels     : {LABELS_OUT}\")\n",
    "print(\"Sanity forward OK. logits shape:\", tuple(y.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
